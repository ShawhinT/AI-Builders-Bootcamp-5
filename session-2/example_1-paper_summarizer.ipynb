{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4ef775-778d-4c0f-b901-65df2ba5caf5",
   "metadata": {},
   "source": [
    "# Summarizing Research Papers with GPT-4.1\n",
    "## ABB #5 - Session 2\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf11ab2-418e-4ea3-a3b8-2d2a09232c88",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b91857-1b75-4b5e-ac03-0bb05bf9e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db523c7-0b98-402d-91c4-d5e4c21c2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sk from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# connect to openai API\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d88996-64c0-43b0-af2a-0a4af4f46152",
   "metadata": {},
   "source": [
    "### 1) Extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06f8b84-23cf-49c3-a024-ebf98b1b8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"content/attention-is-all-you-need.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42881f0-32bf-4733-809a-b290118c9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = fitz.open(filepath)\n",
    "text = \"\".join([page.get_text() for page in pdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb264250-d62c-472b-ad3e-21bbd263551e",
   "metadata": {},
   "source": [
    "### 2) Write prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aaab4a9-cfe5-4488-87b3-c173fc96320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"# Role and Objective\n",
    "\n",
    "You are a research assistant. Your task is to read technical articles and condense them into concise, clear, and accessible summaries. \n",
    "\n",
    "# Instructions\n",
    "\n",
    "Review the article provided by the user and generate a summary in the following format.\n",
    "\n",
    "- **Title**: The paper's title.\n",
    "- **Executive Summary**: The research goal or question being addressed.\n",
    "- **Key Terms**: Technical terms and concepts central to the article's contents\n",
    "    - Term 1 = term 1 definition in context of article\n",
    "    - Term 2 = term 2 definition in context of article\n",
    "    - ...\n",
    "- **Key Findings**: Most critical findings and insights of the paper relevant to AI researchers and engineers.\n",
    "- **Further Reading**: References which are foundational to article's work or dive deeper into key concepts.\n",
    "\n",
    "# Guidelines\n",
    "\n",
    "- Write in a neutral and academic tone.\n",
    "- Use simple, precise language to ensure clarity for a broad audience.\n",
    "- Keep summaries concise (150-300 words) unless otherwise specified.\n",
    "- Assume the audience has general technical knowledge but may not be familiar with the specific field of the paper.\n",
    "- Please limit bullets under key terms, key findings, and further reading, sections to the 3-5 most essential.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6bf9ab6-f80b-4720-9f1a-d4fdbca66af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Role and Objective\n",
      "\n",
      "You are a research assistant. Your task is to read technical articles and condense them into concise, clear, and accessible summaries. \n",
      "\n",
      "# Instructions\n",
      "\n",
      "Review the article provided by the user and generate a summary in the following format.\n",
      "\n",
      "- **Title**: The paper's title.\n",
      "- **Executive Summary**: The research goal or question being addressed.\n",
      "- **Key Terms**: Technical terms and concepts central to the article's contents\n",
      "    - Term 1 = term 1 definition in context of article\n",
      "    - Term 2 = term 2 definition in context of article\n",
      "    - ...\n",
      "- **Key Findings**: Most critical findings and insights of the paper relevant to AI researchers and engineers.\n",
      "- **Further Reading**: References which are foundational to article's work or dive deeper into key concepts.\n",
      "\n",
      "# Guidelines\n",
      "\n",
      "- Write in a neutral and academic tone.\n",
      "- Use simple, precise language to ensure clarity for a broad audience.\n",
      "- Keep summaries concise (150-300 words) unless otherwise specified.\n",
      "- Assume the audience has general technical knowledge but may not be familiar with the specific field of the paper.\n",
      "- Please limit bullets under key terms, key findings, and further reading, sections to the 3-5 most essential.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc17b34-940f-4ff3-ad67-3b2fd7ee5a48",
   "metadata": {},
   "source": [
    "### 3) Summarize Paper with GPT-4o\n",
    "\n",
    "Responses API: https://platform.openai.com/docs/api-reference/responses/create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a27209d-5711-4eab-9828-97d301d17bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Title**: Attention Is All You Need\n",
      "\n",
      "- **Executive Summary**:  \n",
      "  This paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks such as machine translation. Unlike previous models, it dispenses completely with recurrence and convolution, instead relying entirely on attention mechanismsâ€”specifically, self-attention and multi-head attention. The Transformer achieves state-of-the-art results in machine translation (notably on WMT 2014 English-to-German and English-to-French) with much greater computational efficiency, parallelizability, and significantly reduced training time compared to prior approaches. The model also generalizes well to other sequence tasks, demonstrated by its strong performance in English constituency parsing.\n",
      "\n",
      "- **Key Terms**:\n",
      "    - **Transformer**: A neural network architecture for sequence-to-sequence tasks that employs solely attention mechanisms (no RNNs or CNNs), featuring stacked self-attention and feed-forward layers in both encoder and decoder.\n",
      "    - **Self-Attention**: An attention mechanism relating different positions of a single input sequence to compute contextual representations, enabling the model to capture dependencies irrespective of their distance in the sequence.\n",
      "    - **Multi-Head Attention**: Multiple parallel attention mechanisms (heads) allowing the model to jointly focus on information from different subspaces at different positions.\n",
      "    - **Positional Encoding**: Injects sequence order information into embeddings, typically using sinusoidal functions, since the Transformer has no recurrence or convolution to model order.\n",
      "    - **Sequence Transduction**: The transformation of an input sequence into an output sequence (e.g., machine translation).\n",
      "\n",
      "- **Key Findings**:\n",
      "    - The Transformer outperforms state-of-the-art models (including ensembles) in English-German and English-French machine translation, achieving higher BLEU scores (28.4 and 41.8 respectively) with dramatically lower training costs.\n",
      "    - The model can be efficiently trained due to its parallelizable architecture, reducing training time from days/weeks to hours.\n",
      "    - Self-attention enables shorter paths for information flow, making it easier to capture long-range dependencies compared to RNN- or CNN-based models.\n",
      "    - The architecture generalizes beyond translation, achieving strong results in English constituency parsing with minimal task-specific tuning.\n",
      "    - Attention mechanisms within the model can be interpreted: different attention heads learn to focus on different linguistic phenomena (such as syntax, co-reference).\n",
      "\n",
      "- **Further Reading**:\n",
      "    - [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. (Attention mechanism in NMT.)\n",
      "    - [9] Jonas Gehring et al. Convolutional sequence to sequence learning. (CNN-based sequence models.)\n",
      "    - [35] Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. (RNN encoder-decoder models.)\n",
      "    - [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. (Optimization algorithm.)\n",
      "    - [38] Yonghui Wu et al. Google's neural machine translation system: Bridging the gap between human and machine translation. (Large-scale NMT.)\n",
      "CPU times: user 99.5 ms, sys: 23.8 ms, total: 123 ms\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# make api call\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    instructions=prompt,\n",
    "    input=text\n",
    ")\n",
    "\n",
    "# extract response\n",
    "summary = response.output_text\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e89e07-f2d5-4eec-8f83-ba8da7fb2772",
   "metadata": {},
   "source": [
    "### 4) Display Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a058e3ea-480e-4f61-96cf-ddfced60c94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Title**: Attention Is All You Need\n",
       "\n",
       "- **Executive Summary**:  \n",
       "  This paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks such as machine translation. Unlike previous models, it dispenses completely with recurrence and convolution, instead relying entirely on attention mechanismsâ€”specifically, self-attention and multi-head attention. The Transformer achieves state-of-the-art results in machine translation (notably on WMT 2014 English-to-German and English-to-French) with much greater computational efficiency, parallelizability, and significantly reduced training time compared to prior approaches. The model also generalizes well to other sequence tasks, demonstrated by its strong performance in English constituency parsing.\n",
       "\n",
       "- **Key Terms**:\n",
       "    - **Transformer**: A neural network architecture for sequence-to-sequence tasks that employs solely attention mechanisms (no RNNs or CNNs), featuring stacked self-attention and feed-forward layers in both encoder and decoder.\n",
       "    - **Self-Attention**: An attention mechanism relating different positions of a single input sequence to compute contextual representations, enabling the model to capture dependencies irrespective of their distance in the sequence.\n",
       "    - **Multi-Head Attention**: Multiple parallel attention mechanisms (heads) allowing the model to jointly focus on information from different subspaces at different positions.\n",
       "    - **Positional Encoding**: Injects sequence order information into embeddings, typically using sinusoidal functions, since the Transformer has no recurrence or convolution to model order.\n",
       "    - **Sequence Transduction**: The transformation of an input sequence into an output sequence (e.g., machine translation).\n",
       "\n",
       "- **Key Findings**:\n",
       "    - The Transformer outperforms state-of-the-art models (including ensembles) in English-German and English-French machine translation, achieving higher BLEU scores (28.4 and 41.8 respectively) with dramatically lower training costs.\n",
       "    - The model can be efficiently trained due to its parallelizable architecture, reducing training time from days/weeks to hours.\n",
       "    - Self-attention enables shorter paths for information flow, making it easier to capture long-range dependencies compared to RNN- or CNN-based models.\n",
       "    - The architecture generalizes beyond translation, achieving strong results in English constituency parsing with minimal task-specific tuning.\n",
       "    - Attention mechanisms within the model can be interpreted: different attention heads learn to focus on different linguistic phenomena (such as syntax, co-reference).\n",
       "\n",
       "- **Further Reading**:\n",
       "    - [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. (Attention mechanism in NMT.)\n",
       "    - [9] Jonas Gehring et al. Convolutional sequence to sequence learning. (CNN-based sequence models.)\n",
       "    - [35] Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. (RNN encoder-decoder models.)\n",
       "    - [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. (Optimization algorithm.)\n",
       "    - [38] Yonghui Wu et al. Google's neural machine translation system: Bridging the gap between human and machine translation. (Large-scale NMT.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aad5dc-bbc8-4ac9-8421-1e9ae83fcdc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
